# AI Newsletter Research Results - Research Papers

## Featured Research Papers

1. **Name of the Paper**: Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs

**Publisher:** arXiv

**Description:** This paper investigates the fundamental reasoning abilities of Large Language Models (LLMs), specifically differentiating between inductive and deductive reasoning. The authors propose a novel framework, SolverLearner, to isolate and evaluate the inductive reasoning capabilities of LLMs. Their findings suggest that while LLMs exhibit strong inductive reasoning, they tend to lack in deductive reasoning, particularly in tasks involving "counterfactual" reasoning.

**Possible Impact:** This research could lead to a better understanding of the reasoning capabilities and limitations of LLMs, which could inform the development of more robust and reliable AI systems. It also highlights the need for more nuanced evaluation of LLM reasoning, moving beyond a single "reasoning" metric.

**Download Link:** [http://arxiv.org/pdf/2408.00114v2](http://arxiv.org/pdf/2408.00114v2)

2. **Name of the Paper**: DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search

**Publisher:** arXiv

**Description:** This paper proposes a new approach called DOTS (Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search) to enhance the reasoning capabilities of LLMs. Instead of using static, predefined reasoning strategies, DOTS enables LLMs to dynamically choose the best reasoning "actions" for a given question. This is achieved by searching for the optimal sequence of reasoning steps for each question and then training the LLM to plan these trajectories for new questions.

**Possible Impact:** DOTS could lead to more efficient and effective reasoning in LLMs, as they would be able to adapt their reasoning process to the specific problem at hand. This could improve their performance on a wide range of reasoning tasks and make them more flexible and powerful problem-solvers.

**Download Link:** [http://arxiv.org/pdf/2410.03864v2](http://arxiv.org/pdf/2410.03864v2)

3. **Name of the Paper**: MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs

**Publisher:** arXiv

**Description:** This paper introduces MultiNRC (Multilingual Native Reasoning Challenge), a new benchmark for evaluating the multilingual reasoning capabilities of LLMs. Unlike existing benchmarks that are often translated from English, MultiNRC consists of over 1,000 native, culturally-grounded reasoning questions in French, Spanish, and Chinese. The benchmark covers linguistic reasoning, wordplay, cultural reasoning, and math problems with cultural relevance.

**Possible Impact:** MultiNRC provides a much-needed tool for assessing the true multilingual and multicultural reasoning abilities of LLMs. The results from this benchmark could highlight the current limitations of LLMs in these areas and drive the development of models that are more globally competent and culturally aware.

**Download Link:** [http://arxiv.org/pdf/2507.17476v1](http://arxiv.org/pdf/2507.17476v1)
